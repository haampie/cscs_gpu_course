{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8435688d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/Julia/doc/cscs_gpu_course/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(@__DIR__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a459ca",
   "metadata": {},
   "source": [
    "# Programming models\n",
    "\n",
    "There are different ways of programming (NVIDIA) GPUs in Julia, at different levels of abstraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e37034",
   "metadata": {},
   "source": [
    "## Array programming\n",
    "\n",
    "The easiest way to use a GPU is via vectorized array operations. Each of these operations will be backed by one or more GPU kernels, so as long as your data is large enough you'll get to see some nice speed-ups. For NVIDIA GPUs, you use the `CuArray` type from CUDA.jl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae46b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CUDA\n",
    "CuArray([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa3ec71",
   "metadata": {},
   "source": [
    "A `CuArray` is an object that can be used on the CPU, representing a chunk of memory on the GPU. This is important: All operations on a `CuArray` are CPU methods which launch on or more GPU kernels operating on the values in GPU memory. This has several consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57652dee",
   "metadata": {},
   "source": [
    "### Scalar iteration is slow\n",
    "\n",
    "Because `CuArray` operations start on the CPU, the array operations should be relatively heavyweight to offset the overhead it takes to launch one or more GPU operations. That means that a scalar `for` loop processing one element at a time is very wasteful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba2b8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Performing scalar indexing on task Task (runnable) @0x00007fc5b0941510.\n",
      "│ Invocation of getindex resulted in scalar indexing of a GPU array.\n",
      "│ This is typically caused by calling an iterating implementation of a method.\n",
      "│ Such implementations *do not* execute on the GPU, but very slowly on the CPU,\n",
      "│ and therefore are only permitted from the REPL for prototyping purposes.\n",
      "│ If you did intend to index this array, annotate the caller with @allowscalar.\n",
      "└ @ GPUArrays /home/tim/Julia/depot/packages/GPUArrays/3sW6s/src/host/indexing.jl:56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CuArray(1:10)\n",
    "A_sum = zero(eltype(A))\n",
    "for I in eachindex(A)\n",
    "    A_sum += A[I]\n",
    "end\n",
    "A_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee038f2c",
   "metadata": {},
   "source": [
    "Because of this kind of programming pattern, iterating the array and fetching one scalar at a time (hence 'scalar iteration'), being so slow CUDA.jl warns about it. With the above snippet, the situation is actually even worse: Not only does every iteration require a GPU operation to fetch an element, the `getindex` call is also the only array operation meaning that the actual summation won't even run on the GPU!\n",
    "\n",
    "The solution here is to use the `sum` function that performs the entire operation as a single step. More on these operations later.\n",
    "To disallow scalar iteration, use the `allowscalar` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "303d3f85",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore are only permitted from the REPL for prototyping purposes.\nIf you did intend to index this array, annotate the caller with @allowscalar.",
     "output_type": "error",
     "traceback": [
      "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore are only permitted from the REPL for prototyping purposes.\nIf you did intend to index this array, annotate the caller with @allowscalar.",
      "",
      "Stacktrace:",
      " [1] error(s::String)",
      "   @ Base ./error.jl:33",
      " [2] assertscalar(op::String)",
      "   @ GPUArrays ~/Julia/depot/packages/GPUArrays/3sW6s/src/host/indexing.jl:53",
      " [3] getindex(xs::CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}, I::Int64)",
      "   @ GPUArrays ~/Julia/depot/packages/GPUArrays/3sW6s/src/host/indexing.jl:86",
      " [4] top-level scope",
      "   @ In[3]:2",
      " [5] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [6] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "CUDA.allowscalar(false)\n",
    "A[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194c612d",
   "metadata": {},
   "source": [
    "### CuArray isn't device-compatible\n",
    "\n",
    "A more subtle result of `CuArray` being the CPU-side object is that these objects cannot be used directly on the GPU. Instead, a conversion to `CuDeviceArray` happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd79553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTX CompilerJob of kernel #2(CuDeviceVector{Int64, 1}) for sm_75\n",
      "\n",
      "Variables\n",
      "  #self#\u001b[36m::Core.Const(var\"#2#3\"())\u001b[39m\n",
      "  A\u001b[36m::CuDeviceVector{Int64, 1}\u001b[39m\n",
      "\n",
      "Body\u001b[36m::Nothing\u001b[39m\n",
      "\u001b[90m1 ─\u001b[39m     return Main.nothing\n"
     ]
    }
   ],
   "source": [
    "@device_code_warntype @cuda (A->nothing)(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c8fd2d",
   "metadata": {},
   "source": [
    "Typically, this conversion is hidden and shouldn't affect you as an end user. The only time you need to take care, is when embedding `CuArray`s in a structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c30c38c",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "GPU compilation of kernel #4(MyStruct) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 2 to your kernel function is of type MyStruct, which is not isbits:\n  .inner is of type CuArray which is not isbits.\n    .storage is of type Union{Nothing, CUDA.ArrayStorage{B}} where B which is not isbits.\n    .dims is of type Tuple{Vararg{Int64, N}} where N which is not isbits.\n\n",
     "output_type": "error",
     "traceback": [
      "GPU compilation of kernel #4(MyStruct) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 2 to your kernel function is of type MyStruct, which is not isbits:\n  .inner is of type CuArray which is not isbits.\n    .storage is of type Union{Nothing, CUDA.ArrayStorage{B}} where B which is not isbits.\n    .dims is of type Tuple{Vararg{Int64, N}} where N which is not isbits.\n\n",
      "",
      "Stacktrace:",
      "  [1] check_invocation(job::GPUCompiler.CompilerJob)",
      "    @ GPUCompiler ~/Julia/depot/packages/GPUCompiler/AJD5L/src/validation.jl:66",
      "  [2] macro expansion",
      "    @ ~/Julia/depot/packages/GPUCompiler/AJD5L/src/driver.jl:332 [inlined]",
      "  [3] macro expansion",
      "    @ ~/Julia/depot/packages/TimerOutputs/SSeq1/src/TimerOutput.jl:252 [inlined]",
      "  [4] macro expansion",
      "    @ ~/Julia/depot/packages/GPUCompiler/AJD5L/src/driver.jl:331 [inlined]",
      "  [5] emit_asm(job::GPUCompiler.CompilerJob, ir::LLVM.Module; strip::Bool, validate::Bool, format::LLVM.API.LLVMCodeGenFileType)",
      "    @ GPUCompiler ~/Julia/depot/packages/GPUCompiler/AJD5L/src/utils.jl:62",
      "  [6] cufunction_compile(job::GPUCompiler.CompilerJob)",
      "    @ CUDA ~/Julia/depot/packages/CUDA/P3AyQ/src/compiler/execution.jl:326",
      "  [7] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link))",
      "    @ GPUCompiler ~/Julia/depot/packages/GPUCompiler/AJD5L/src/cache.jl:89",
      "  [8] cufunction(f::var\"#4#5\", tt::Type{Tuple{MyStruct}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ CUDA ~/Julia/depot/packages/CUDA/P3AyQ/src/compiler/execution.jl:297",
      "  [9] cufunction(f::var\"#4#5\", tt::Type{Tuple{MyStruct}})",
      "    @ CUDA ~/Julia/depot/packages/CUDA/P3AyQ/src/compiler/execution.jl:291",
      " [10] top-level scope",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/src/compiler/execution.jl:102",
      " [11] eval",
      "    @ ./boot.jl:360 [inlined]",
      " [12] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "struct MyStruct\n",
    "    inner::CuArray\n",
    "end\n",
    "B = MyStruct(A)\n",
    "@cuda (A->nothing)(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f4aaa",
   "metadata": {},
   "source": [
    "Here, CUDA.jl makes it clear that a `CuArray` isn't GPU compatible because it's not an `isbits` type. The underlying reason is that the automatic conversion from `CuArray` to `CuDeviceArray` doesn't know about your `MyStruct` and how to convert it to something GPU-compatible. This conversion is done using Adapt.jl, and to make this code work you need to teach Adapt about how to convert `MyStruct` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccabb456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTX CompilerJob of kernel #6(MyParametricStruct{CuDeviceVector{Int64, 1}}) for sm_75\n",
      "\n",
      "Variables\n",
      "  #self#\u001b[36m::Core.Const(var\"#6#7\"())\u001b[39m\n",
      "  A\u001b[36m::MyParametricStruct{CuDeviceVector{Int64, 1}}\u001b[39m\n",
      "\n",
      "Body\u001b[36m::Nothing\u001b[39m\n",
      "\u001b[90m1 ─\u001b[39m     return Main.nothing\n"
     ]
    }
   ],
   "source": [
    "# to store both a CuArray and a CuDeviceArray\n",
    "# our struct needs to be parametric\n",
    "struct MyParametricStruct{T<:AbstractArray}\n",
    "    inner::T\n",
    "end\n",
    "\n",
    "using Adapt\n",
    "Adapt.adapt_structure(to, x::MyParametricStruct) = MyParametricStruct(adapt(to, x.inner))\n",
    "\n",
    "C = MyParametricStruct(A)\n",
    "@device_code_warntype @cuda (A->nothing)(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84d3543",
   "metadata": {},
   "source": [
    "### Array operations\n",
    "\n",
    "Many array operations from Julia's standard library and package ecosystem Just Work, because they rely on the `AbstractArray` interfaces that are implemented by `CuArray`. Still, CUDA.jl specializes a bunch of methods for several reasons:\n",
    "\n",
    "- compatibility: to avoid scalar iteration, or calling into a C library with CPU code (e.g. BLAS, LAPACK, ...)\n",
    "- performance: by using GPU-optimized implementations, either implemented in Julia or in vendor libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c753e85",
   "metadata": {},
   "source": [
    "If an array operation isn't available, two kinds of errors might occur:\n",
    "\n",
    "- scalar iteration: when the default implementation processes individual elements at a time\n",
    "- invalid conversions to a CPU pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2c6005b",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: cannot take the CPU address of a CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}",
     "output_type": "error",
     "traceback": [
      "ArgumentError: cannot take the CPU address of a CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}",
      "",
      "Stacktrace:",
      " [1] unsafe_convert(#unused#::Type{Ptr{Int64}}, x::CuArray{Int64, 1, CUDA.Mem.DeviceBuffer})",
      "   @ CUDA ~/Julia/depot/packages/CUDA/P3AyQ/src/array.jl:315",
      " [2] unsafe_convert(#unused#::Type{Ptr{Float32}}, a::CuArray{Int64, 1, CUDA.Mem.DeviceBuffer})",
      "   @ Base ./pointer.jl:66",
      " [3] top-level scope",
      "   @ ./In[7]:1",
      " [4] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "ccall(:whatever, Nothing, (Ptr{Float32},), A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c3981",
   "metadata": {},
   "source": [
    "In that case, either you need to use different (supported) array operations, or fix the implementation in CUDA.jl. Such a fix can mean using functions from a CUDA library, using existing operations, or writing your own kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c166fc3e",
   "metadata": {},
   "source": [
    "### Exercise: Matrix RMSE\n",
    "\n",
    "As an trivial exercise, try to compute the RMSE of two matrices on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85ee738d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4083871458683567"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(1024, 1024)\n",
    "B = rand(1024, 1024)\n",
    "sqrt(sum((A-B).^2) / length(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50845c3",
   "metadata": {},
   "source": [
    "Easy enough, just changing the type of the input arrays to `CuArray` and the computation of C just works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1defb88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4083871458683567"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CuArray(A)\n",
    "B = CuArray(B)\n",
    "sqrt(sum((A-B).^2) / length(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1447015",
   "metadata": {},
   "source": [
    "This is of course a trivial example, but let's keep it simple for now. The next notebooks will start from this example and create something realistic out of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ecb474",
   "metadata": {},
   "source": [
    "## Kernel programming\n",
    "\n",
    "Kernels are scalar functions that get executed multiple times in parallel. Each 'thread' runs on one of the many streaming multiprocessors a GPU has, and threads running on a single SM are called a 'block'. Within a SM, some threads are always executed together; these form a 'warp' of 32 threads. Efficient communication between these entities is required to effectively use the GPU:\n",
    "\n",
    "- between blocks: global memory\n",
    "- within a block: shared memory\n",
    "- within a warp: via registers (shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1e4c71",
   "metadata": {},
   "source": [
    "### Hardware indices\n",
    "\n",
    "You can fetch the thread, block and warp index using specific functions that query hardware indices:\n",
    "\n",
    "- `threadIdx()` and `blockDim()`: 3D\n",
    "- `blockIdx()` and `gridDim()`: 3D\n",
    "- `laneid()` and `warpsize()`\n",
    "\n",
    "When you don't need to care about which block a thread is part of, a very common index calculation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf386c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.HostKernel{typeof(kernel), Tuple{}}(kernel, CuContext(0x00000000062264a0, instance f02fca961a36c4ad), CuModule(Ptr{Nothing} @0x0000000009d56840, CuContext(0x00000000062264a0, instance f02fca961a36c4ad)), CuFunction(Ptr{Nothing} @0x00000000094ce9c0, CuModule(Ptr{Nothing} @0x0000000009d56840, CuContext(0x00000000062264a0, instance f02fca961a36c4ad))), CUDA.KernelState(Ptr{Nothing} @0x00007fc560c00000))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 1\n",
      "i = 2\n",
      "i = 3\n",
      "i = 4\n"
     ]
    }
   ],
   "source": [
    "function kernel()\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @cushow i\n",
    "    return\n",
    "end\n",
    "@cuda threads=2 blocks=2 kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4247070",
   "metadata": {},
   "source": [
    "### Synchronization\n",
    "\n",
    "If threads are working together -- say, they are using the same global memory, or are communicating using shared memory or finer-grained intrinsics -- you may want to have threads on each other. Note that this is only possible **within a block**; different blocks generally cannot wait on one another.\n",
    "\n",
    "Let's look at a contrived example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ff4d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float32}:\n",
       " 42.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CUDA.zeros(512)\n",
    "\n",
    "function kernel(A)\n",
    "    # simple kernel without multiple blocks\n",
    "    i = threadIdx().x\n",
    "    \n",
    "    # first thread sets up the data\n",
    "    if i == 1\n",
    "        A[1] = 42\n",
    "    end\n",
    "    \n",
    "    sync_threads()\n",
    "    \n",
    "    # other threads can now read this data\n",
    "    if i != 1\n",
    "        A[i] = A[1]\n",
    "    end\n",
    "    \n",
    "    return\n",
    "end\n",
    "@cuda threads=length(A) kernel(A)\n",
    "unique(Array(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f23cc",
   "metadata": {},
   "source": [
    "Note how we didn't put `sync_threads()` inside of the branch; All threads need to reach the synchronization point for the kernel to make progress. This makes it dangerous to synchronize from a branch, as the branch cannot be divergent within a block or the kernel would deadlock!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082f21b3",
   "metadata": {},
   "source": [
    "When coordinating within the warp, you may need the `sync_warp()` function. A detailed explanation of warp-level programming is out of scope for this notebook, refer to the [NVIDIA developer blog](https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade663de",
   "metadata": {},
   "source": [
    "### Atomic operations\n",
    "\n",
    "When you want to use the same global memory from different threads, you may want to use atomic operations. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb9d0979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255.92522f0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum = CUDA.zeros(1)\n",
    "A = CUDA.rand(512)\n",
    "\n",
    "function kernel(A, A_sum)\n",
    "    i = threadIdx().x\n",
    "    CUDA.@atomic A_sum[] += A[i]\n",
    "    return\n",
    "end\n",
    "@cuda threads=length(A) kernel(A, A_sum)\n",
    "Array(A_sum)[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3e656",
   "metadata": {},
   "source": [
    "You shouldn't overuse atomics though, as they generally serialize execution and thus are very expensive! But they may be useful for an initial implementation (i.e. before considering more fine-grained communication), or to reduce values from different blocks (because of the difficulty of synchronizing the grid)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5e5b7",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "To help with implementing a kernel, there's a couple of helpful macros to generate output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f880a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function kernel()\n",
    "    i = threadIdx().x\n",
    "    @cuprintf \"I'm thread %ld\\n\" Int(i)\n",
    "    return\n",
    "end\n",
    "@cuda kernel();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cfed74",
   "metadata": {},
   "source": [
    "However, `@cuprintf` is a bit cumbersome, so we have `@cuprintln` trying to automatically generate an appropriate formatting string, while even supporting string interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0804fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm thread 1\n"
     ]
    }
   ],
   "source": [
    "function kernel()\n",
    "    i = threadIdx().x\n",
    "    @cuprintln \"I'm thread $i\"\n",
    "    return\n",
    "end\n",
    "@cuda kernel();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6add9d3",
   "metadata": {},
   "source": [
    "And for quick debugging, we have a helpful `@cushow` you can surround expressions with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "110e15db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm thread 1\n"
     ]
    }
   ],
   "source": [
    "function kernel()\n",
    "    i = @cushow(threadIdx().x)\n",
    "    return\n",
    "end\n",
    "@cuda kernel();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa97414d",
   "metadata": {},
   "source": [
    "### Exercise: Matrix RMSE kernel\n",
    "\n",
    "We now have all the pieces needed to port our RMSE calculation to a kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fa8342d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(threadIdx()).x = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41447648f0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CUDA.rand(10,10)\n",
    "B = CUDA.rand(10,10)\n",
    "sqrt(sum((A-B).^2)/length(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8d1cf",
   "metadata": {},
   "source": [
    "Try to write a kernel that takes a single-item output array as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "005711b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = CUDA.similar(A, 1)\n",
    "\n",
    "function rmse_kernel(C, A, B)\n",
    "    # ...\n",
    "    return\n",
    "end\n",
    "\n",
    "@cuda threads=length(A) rmse_kernel(C, A, B)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e2abb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.4144765"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rmse_kernel(C, A, B)  \n",
    "    i = threadIdx().x\n",
    "\n",
    "    # initialize the memory\n",
    "    if i == 1\n",
    "        C[] = 0\n",
    "    end\n",
    "    sync_threads()\n",
    "    \n",
    "    # process an element on each thread\n",
    "    a = A[i]\n",
    "    b = B[i]\n",
    "    CUDA.@atomic C[] += (a-b)^2\n",
    "    sync_threads()\n",
    "    \n",
    "    # finalize the computation\n",
    "    if i == 1\n",
    "        C[1] = sqrt(C[] / length(A))\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "@cuda threads=length(A) rmse_kernel(C, A, B)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e4dba",
   "metadata": {},
   "source": [
    "## High-level kernel programming\n",
    "\n",
    "There's a couple of packages that aim to simplify kernel programming without resorting to array operations (which may result in extraneous kernel launches, more on that in some of the next notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3946951a",
   "metadata": {},
   "source": [
    "### Tullio.jl\n",
    "\n",
    "With Tullio, it's easy to write kernels using index notation. This makes it easy to express operations like our RMSE calculation in a single expression which typically will also be compiled to a single kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29407d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-dimensional Array{Float64, 0}:\n",
       "0.40783335160700945"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Tullio\n",
    "\n",
    "A = rand(1024, 1024)\n",
    "B = rand(1024, 1024)\n",
    "@tullio C[] := (A[i,j] - B[i,j])^2 |> sqrt(_ / length(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10447dfc",
   "metadata": {},
   "source": [
    "Any indices used in the right hand-side expression but not in the output array will be reduced. The pipe operator can be used to perform operations outside of the loop, such as here to calculate the mean and take the square root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cc4201",
   "metadata": {},
   "source": [
    "To use Tullio with GPU arrays, you need to install and import the relevant CUDA support packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fc9b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA, CUDAKernels, KernelAbstractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8463dec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-dimensional CuArray{Float32, 0, CUDA.Mem.DeviceBuffer}:\n",
       "0.4084926"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CUDA.rand(1024, 1024)\n",
    "B = CUDA.rand(1024, 1024)\n",
    "@tullio C[] := (A[i,j] - B[i,j])^2 |> sqrt(_ / length(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ed8649",
   "metadata": {},
   "source": [
    "Tullio is great for quickly creating portable kernels (CPU, different GPU back-ends) for mathematical operations, and it can be seen as a generalization of broadcast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b863a6",
   "metadata": {},
   "source": [
    "### KernelAbstractions.jl\n",
    "\n",
    "For a more flexible API, i.e. not restricted to Tullio's index notation, but still retaining Tullio's portability, you can consider the KernelAbstractions.jl framework that Tullio.jl is built on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d513082",
   "metadata": {},
   "outputs": [],
   "source": [
    "using KernelAbstractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41110dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kernel function ka_kernel(A)\n",
    "    # simple kernel without multiple blocks\n",
    "    i = @index(Global, Linear)\n",
    "    \n",
    "    # first thread sets up the data\n",
    "    if i == 1\n",
    "        A[1] = 42\n",
    "    end\n",
    "    \n",
    "    @synchronize()\n",
    "    \n",
    "    # other threads can now read this data\n",
    "    if i != 1\n",
    "        A[i] = A[1]\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb65a2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 42.0\n",
       "  0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = zeros(512)\n",
    "\n",
    "the_ka_kernel = ka_kernel(CPU(), 16)\n",
    "event = the_ka_kernel(A, ndrange=size(A))\n",
    "wait(event)\n",
    "unique(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2aee9",
   "metadata": {},
   "source": [
    "The programming interface is now much closer to CUDA.jl's, while retaining platform portability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b00a9893",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA, CUDAKernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "448c0832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float32}:\n",
       " 42.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CUDA.zeros(512)\n",
    "the_ka_kernel = ka_kernel(CUDADevice(), 16)\n",
    "event = the_ka_kernel(A, ndrange=size(A))\n",
    "wait(event)\n",
    "unique(Array(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0252a3bf",
   "metadata": {},
   "source": [
    "The disadvantage of platform portability of course is that KernelAbstraction.jl's feature set is limited to the common denominator of all supported platforms. That means many CUDA features, like atomics or warp-level programming, are not supported. In addition, KernelAbstractions is built on Cassette.jl which will incur a significant compilation cost for nontrivial applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
